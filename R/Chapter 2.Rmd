# SMALL WORLDS AND LARGE WORLDS

## Grid tossing example

In the context of the globe tossing problem, grid approximation works extremely well.
So let’s build a grid approximation for the model we’ve constructed so far. Here is the recipe:

1.  Define the grid. This means you decide how many points to use in estimating the
posterior, and then you make a list of the parameter values on the grid.  
2.  Compute the value of the prior at each parameter value on the grid.  
3.  Compute the likelihood at each parameter value.  
4.  Compute the unstandardized posterior at each parameter value, by multiplying the
prior by the likelihood.  
5.  Finally, standardize the posterior, by dividing each value by the sum of all values.  
In the globe tossing context, here’s the code to complete all five of these steps:  


```{r rcode 2.3}
# define grid
p_grid <- seq( from=0 , to=1 , length.out=20 )
# define prior
prior <- rep( 1 , 20 )
# compute likelihood at each value in grid
likelihood <- dbinom( 6 , size=9 , prob=p_grid ) # 6 is number of waters, 9 is total sample, prob is vector of probalities
# compute product of likelihood and prior
unstd.posterior <- likelihood * prior
# standardize the posterior, so it sums to 1
posterior <- unstd.posterior / sum(unstd.posterior)


```
The above code makes a grid of only 20 points. To display the posterior distribution now:

```{r rcode 2.4}
plot( p_grid , posterior , type="b" ,
xlab="probability of water" , ylab="posterior probability" )
mtext( "20 points" )
```

Experiment with various gid sizes and prior shapes

```{r rcode 2.5}
# define grid
gridsize <- 2000

p_grid <- seq( from=0 , to=1 , length.out= gridsize)
# define prior
prior <- rep( 1, gridsize )

## assume more than half world is water
#prior <- ifelse( p_grid < 0.5 , 0 , 1 ) 2.5

## crazy prior
#prior <- exp( -5*abs( p_grid - 0.5 ) )

# compute likelihood at each value in grid
likelihood <- dbinom(6 , size=9 , prob= p_grid )
# compute product of likelihood and prior
unstd.posterior <- likelihood * prior
# standardize the posterior, so it sums to 1
posterior <- unstd.posterior / sum(unstd.posterior)

plot( p_grid , posterior , type="b" ,
xlab="probability of water" , ylab="posterior probability" )
mtext(paste(gridsize, " points" ))
      
```

##Quadratic approximation

Quadratic approximation is an alternative to grid approximation, which partly overcomes the limiting factor of grid approximation which will very quickly becaome unmanageable when number of parameters increase.

*".. for two parameters approximated by 100 values each, that’s already $100^2$ = 10000 values to compute. For 10 parameters, the grid becomes many billions
of values."*

*"A Gaussian approximation is called “quadratic approximation” because the logarithm of
a Gaussian distribution forms a parabola. And a parabola is a quadratic function. So this
approximation essentially represents any log-posterior with a parabola."*

The Rethninking package provides quadratic approximations in the following form: Mcelreath is keen for the student to get used to writing out the standard notaion for describing models, which makes sense and helps embed the ideas

```{r message=FALSE}
## R code 2.6
options(mc.cores = parallel::detectCores())  ### reccomended option to get best out of STAN


library(rethinking)
globe.qa <- quap(
    alist(
        W ~ dbinom( W+L ,p) ,  # binomial likelihood
        p ~ dunif(0,1)     # uniform prior
    ) ,
    data=list(W=6,L=3) )

# display summary of quadratic approximation
precis( globe.qa )
```

Compare an analytical version  (exact) pf the model to the equivalen quadratic approximation

```{r}
# analytical calculation 2.7
W <- 6
L <- 3
curve( dbeta( x , W+1 , L+1 ) , from=0 , to=1 )
# quadratic approximation
curve( dnorm( x , 0.67 , 0.16 ) , lty=2 , add=TRUE )
```

at small samples the approximation is Meh, but as sample increases approximation does too


*"The quadratic approximation, either with a uniform
prior or with a lot of data, is often equivalent to a maximum likelihood estimate (MLE) and its
standard error. The MLE is a very common non-Bayesian parameter estimate. This correspondence
between a Bayesian approximation and a common non-Bayesian estimator is both a blessing
and a curse. It is a blessing, because it allows us to re-interpret a wide range of published non-Bayesian
model fits in Bayesian terms. It is a curse, because maximum likelihood estimates have some curious
drawbacks, and can share them. We’ll explore these in later chapters."*

## Monte Carlo 

Here is an example of MCMC run in the rethinking package


```{r}
n_samples <- 10000
p <- rep( NA , n_samples )
p[1] <- 0.5
W <- 6
L <- 3
for ( i in 2:n_samples ) {
p_new <- rnorm( 1 , p[i-1] , 0.1 )
if ( p_new < 0 ) p_new <- abs( p_new )
if ( p_new > 1 ) p_new <- 2 - p_new
q0 <- dbinom( W , W+L , p[i-1] )
q1 <- dbinom( W , W+L , p_new )
p[i] <- ifelse( runif(1) < q1/q0 , p_new , p[i-1] )

}
```

and here is the analyical comparison

```{r}
dens( p , xlim=c(0,1) )
curve( dbeta( x , W+1 , L+1 ) , lty=2 , add=TRUE )
```
 not bad!!



